{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d6b418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # PyMuPDF used for PDF manipulation\n",
    "from langchain_core.documents import Document # Document class from LangChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # Text splitter from LangChain\n",
    "from langchain_community.vectorstores import FAISS # FAISS vector store from LangChain Community\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI # Google Generative AI integration\n",
    "import os # OS module for environment variable access\n",
    "from langchain.embeddings import HuggingFaceEmbeddings ## HuggingFace embeddings from LangChain\n",
    "from gtts import gTTS # Google Text-to-Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db71dab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv # Load environment variables from .env file\n",
    "load_dotenv() # Load the .env file\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"]=os.getenv(\"GEMINI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325376da",
   "metadata": {},
   "source": [
    "DATA INGESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e531425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path=\"job.pdf\"\n",
    "doc=fitz.open(pdf_path) # Open the PDF document\n",
    "\n",
    "#storage for the texts documents we extract from the pdf\n",
    "texts=[]\n",
    "#now we will extract the text from the pdf\n",
    "\n",
    "for page_num,page in enumerate(doc):\n",
    "    text=page.get_text() #Extract text from the page\n",
    "\n",
    "    if(text.strip()): # check if text is not empty\n",
    "        metadata={\n",
    "            \"source\":pdf_path,\n",
    "            \"page\":page_num\n",
    "        }\n",
    "        texts.append(Document(page_content=text,metadata=metadata)) #create a Document obejct and append to texts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#text splitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50) # here we define the chunk size and overlap and basically intialize the text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf8dfcfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6741808c",
   "metadata": {},
   "source": [
    "CHUNKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "32594441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=text_splitter.split_documents(texts) # split the documents into chunks\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063c2354",
   "metadata": {},
   "source": [
    "NOW WE NEED TO DO EMMBEDDING OF THE CHUNKS TO STORE IN THE VECTOR DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b75f506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings( # Initialize HuggingFace embeddings\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "#so the thing is we dont need to embed the chunks ourselves because the vector store will do it for us at the time of storing the chunks in the vector db\n",
    "\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=chunks,  #documents to be stored\n",
    "    embedding=embeddings #embedding model to be used which we initialized earlier\n",
    ")\n",
    "\n",
    "print(vector_store.index.ntotal) #number of vectors stored in the vector db and this should be equal to number of chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0595bf2d",
   "metadata": {},
   "source": [
    "SEE THIS PART IS DONE NOW WE MOVE ONTO THE USER QUERY PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f22aa51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1\n",
      "Sample User Flows\n",
      "Flow 1: Manual Task Creation\n",
      "1. User clicks \"Add Task\" button\n",
      "2. A form appears with fields for title, description, status, priority, and due \n",
      "date\n",
      "3. User fills in the fields and clicks \"Save\"\n",
      "4. Task appears in the appropriate column/list\n",
      "Flow 2: Voice Task Creation\n",
      "1. User clicks the microphone icon\n",
      "2. User speaks: \"Remind me to send the project proposal to the client by next \n",
      "Wednesday, it's high priority\"\n",
      "{'source': 'job.pdf', 'page': 3}\n",
      "----\n",
      "Chunk 2\n",
      "b. Assumptions you made (about emails, formats, limitations, etc.).\n",
      "5. AI Tools Usage\n",
      "a. Which AI tools you used while building (Copilot, ChatGPT, Claude, \n",
      "Cursor, etc.).\n",
      "b. What they helped with (boilerplate, debugging, design, parsing ideas, \n",
      "etc.).\n",
      "c. Any notable prompts/approaches.\n",
      "d. What you learned or changed because of these tools.\n",
      "Demo Video (Mandatory)\n",
      "Create a 5-10 minute screen recording that includes:\n",
      "1. Quick walkthrough of the application\n",
      "{'source': 'job.pdf', 'page': 5}\n",
      "----\n",
      "Chunk 3\n",
      "a. Prerequisites (Node version, DB, API keys).\n",
      "b. Install steps (frontend & backend).\n",
      "c. How to configure email sending/receiving.\n",
      "d. How to run everything locally.\n",
      "e. Any seed data or initial scripts.\n",
      "2. Tech Stack\n",
      "a. Frontend, backend, DB, AI provider, email solution, and key libraries.\n",
      "3. API Documentation\n",
      "a. Main endpoints: method + path, request body/params, example \n",
      "success & error responses.\n",
      "4. Decisions & Assumptions\n",
      "a. Key design decisions (models, flows, scoring, etc.).\n",
      "{'source': 'job.pdf', 'page': 5}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# now we give the query part\n",
    "\n",
    "query = \"what do we need to build in the assignments exactly?\"\n",
    "\n",
    "#now we create the retriever from the vector store to retrieve the top k similar chunks from the vector db\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"k\": 3} #number of similar chunks to retrieve\n",
    ")\n",
    "\n",
    "docs = retriever.get_relevant_documents(query) #retrieve the relevant documents and store in docs\n",
    "\n",
    "\n",
    "#just to see what chunks we have retrieved and their metadata\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Chunk {i+1}\")\n",
    "    print(doc.page_content)\n",
    "    print(doc.metadata)\n",
    "    print(\"----\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0247acdb",
   "metadata": {},
   "source": [
    "OK WE HAVE BUILD THE RETRIEVER NOW WE MOVE ONTO THE LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cd30c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, the assignments require building or producing the following:\n",
      "\n",
      "1.  An application that supports user flows such as:\n",
      "    a.  Manual Task Creation, where a user clicks \"Add Task\", fills a form with title, description, status, priority, and due date, clicks \"Save\", and the task appears.\n",
      "    b.  Voice Task Creation, where a user clicks a microphone icon and speaks a command like \"Remind me to send the project proposal to the client by next Wednesday, it's high priority\".\n",
      "2.  Documentation of assumptions made (about emails, formats, limitations, etc.).\n",
      "3.  Documentation of AI Tools Usage, including:\n",
      "    a.  Which AI tools were used (Copilot, ChatGPT, Claude, Cursor, etc.).\n",
      "    b.  What they helped with (boilerplate, debugging, design, parsing ideas, etc.).\n",
      "    c.  Any notable prompts/approaches.\n",
      "    d.  What was learned or changed because of these tools.\n",
      "4.  A 5-10 minute screen recording (Demo Video) that includes:\n",
      "    a.  A quick walkthrough of the application.\n",
      "    b.  Prerequisites (Node version, DB, API keys).\n",
      "    c.  Install steps (frontend & backend).\n",
      "    d.  How to configure email sending/receiving.\n",
      "    e.  How to run everything locally.\n",
      "    f.  Any seed data or initial scripts.\n",
      "    g.  The Tech Stack (Frontend, backend, DB, AI provider, email solution, and key libraries).\n",
      "    h.  API Documentation (Main endpoints: method + path, request body/params, example success & error responses).\n",
      "    i.  Decisions & Assumptions (Key design decisions like models, flows, scoring, etc.).\n"
     ]
    }
   ],
   "source": [
    "llm=ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\", #specify the model\n",
    "    temperature=0, #temperature for response generation\n",
    ")\n",
    "\n",
    "\n",
    "context = \"\\n\".join([doc.page_content for doc in docs]) #combine the retrieved chunks into a single context string\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Answer the question using only the context below.\n",
    "\n",
    "Follow these rules strictly:\n",
    "1. Use numbers (1, 2, 3) for main points.\n",
    "2. Use lowercase alphabets (a, b, c) for sub-points.\n",
    "3. Do not use asterisks, dashes, markdown, or special symbols.\n",
    "4. Write in plain text only.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "answer= llm.invoke(prompt) #generate the answer using the llm\n",
    "answer_text = answer.content\n",
    "\n",
    "print(answer_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f129485",
   "metadata": {},
   "source": [
    "NOW WE MOVE ONTO THE TTS PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "44bffdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio answer saved as answer.mp3\n"
     ]
    }
   ],
   "source": [
    "tts=gTTS(text=answer_text,lang='en') #initialize gTTS with the answer text and language\n",
    "tts.save(\"answer.mp3\") #save the audio file\n",
    "\n",
    "print(\"Audio answer saved as answer.mp3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
